{"api":{"version":2,"latestVersion":2},"lesson":{"id":"chaines-markov","level":"terminale","title":"Chapitre XVI – Chaînes de Markov (Maths expertes)","chapter":16,"specialty":true,"content":"/api/v2/terminale/chaines-markov/","comments":"/api/v2/terminale/chaines-markov/comments/","summary":"/api/v2/terminale/chaines-markov/summary/"},"html":"<h2 id=\"graphe-pondéré-et-graphe-probabiliste\">Graphe pondéré et graphe probabiliste</h2>\n<h3 id=\"définition\">Définition</h3>\n<div class=\"formula\">\n<h4>Graphe pondéré</h4>\n<p>Un graphe est dit <strong>pondéré</strong> si chacune de ses arêtes est affecté d’un nombre positif (ou nul) que l’on appelle <strong>poids</strong>.</p>\n<p>Le poids d’une chaîne (ou d’un chemin) est la somme des poids de ses arêtes.</p>\n</div>\n\n<div class=\"formula\">\n<h4>Graphe probabiliste</h4>\n<p>On appelle <strong>graphe probabiliste</strong> un graphe orienté et pondéré tel que :</p>\n<ul>\n<li><p>Pour chaque sommet, la somme des poids des arcs issus de ce sommet vaut $1$.</p></li>\n<li><p>Il y a au plus $1$ arrête orientée reliant chaque sommet.</p></li>\n</ul>\n</div>\n<p>Il peut être utile de faire l’analogie entre les graphes probabilistes et <a href=\"https://bacomathiqu.es/cours/premiere/probabilites/#arbre-de-probabilit%C3%A9\">les arbres de probabilité</a> vus en classe de Première.</p>\n\n<h3 id=\"matrice-de-transition\">Matrice de transition</h3>\n<div class=\"formula\">\n<h4>Définition</h4>\n<p>Soit $G$ un graphe probabiliste d’ordre $n$. On appelle <strong>matrice de transition</strong> du graphe $G$, la matrice carrée d’ordre $n$ dont le coefficient à la ligne $i$ et à la colonne $j$ est égal au poids de l’arête reliant le sommet $i$ au sommet $j$.</p>\n<p>Une telle matrice est qualifiée de <strong>stochastique</strong> car la somme des coefficients de chacune de ses lignes vaut $1$.</p>\n</div>\n\n<p>Attention cependant à ne pas confondre matrice de transition et matrice d’adjacence.</p>\n<h2 id=\"chaînes-de-markov\">Chaînes de Markov</h2>\n<h3 id=\"quest-ce-quune-chaîne-de-markov-\">Qu’est-ce qu’une chaîne de Markov ?</h3>\n<p>Il vous est fortement conseillé de relire (et de maîtriser) le cours sur <a href=\"https://bacomathiqu.es/cours/terminale/variables-aleatoires-concentration-grands-nombres/\">les variables aléatoires</a> avant d’aborder cette section. De plus, sachez que cette partie est sans doute la plus difficile du programme de Terminale. Mais ne vous découragez pas car elle reste parfaitement accessible !</p>\n<div class=\"formula\">\n<h4>Définition</h4>\n<p>Soit $(X_n)$ une suite de variables aléatoires discrètes définies sur un même univers $\\Omega$ et à valeurs dans un ensemble $E$. On dit que $(X_n)$ définit une <strong>chaîne de Markov</strong> sur $E$ si pour tout $n \\in \\mathbb{N}$ et tout $x_0, x_1, x_2, \\dots, x_n \\in E$, l’événement $(X_n = x_n)$ ne dépend que de l’événement antérieur $(X_{n-1} = x_{n-1})$ (et pas des précédents) ; autrement dit, si $P_{(X_{n-1} = x_{n-1}) \\, \\cap \\dots \\cap \\, (X_0 = x_0)}(X_n = x_n) = P_{(X_{n-1} = x_{n-1})}(X_n = x_n)$.</p>\n<p>De plus, l’ensemble $E$ est appelé <strong>espace des états</strong> de la chaîne de Markov.</p>\n</div>\n<p>En français, cela signifie que si $X_n$ représente l’état d’un système à un temps $n$, alors l’état suivant $X_{n+1}$ ne dépend que de l’état au temps $n$ et pas des états précédents. De plus, notez bien que nous n’avons pas fait d’hypothèse sur le cardinal de $E$ (qui peut donc être de cardinal $m \\in \\mathbb{N}$).</p>\n\n\n<div class=\"formula\">\n<h4>Chaîne de Markov homogène</h4>\n<p>Soit $(X_n)$ une chaîne de Markov dont on note $E$ l’espace des états. Alors $(X_n)$ est dite <strong>homogène</strong> si pour tout $n \\in \\mathbb{N}$ et pour tout $x$, $y \\in E$, la probabilité $P_{(X_n = x)}(X_{n+1} = y)$ est indépendante de $n$.</p>\n<p>En termes mathématiques, cela signifie que pour tout $n \\in \\mathbb{N}$ et pour tout $x$, $y \\in E$, $P_{(X_n = x)}(X_{n+1} = y) = P_{(X_0 = x)}(X_1 = y)$.</p>\n</div>\n\n<h3 id=\"matrice-et-graphe-associés-à-une-chaîne-de-markov\">Matrice et graphe associés à une chaîne de Markov</h3>\n<div class=\"formula\">\n<h4>Matrice de transition</h4>\n<p>Soit $(X_n)$ une chaîne de Markov homogène dont on note $E = \\{x_1, x_2, \\dots, x_m\\}$ l’espace des états. La <strong>matrice de transition</strong> de $(X_n)$ est la matrice carrée d’ordre $m$ dont le coefficient situé à la $i$-ième ligne et à la $j$-ième colonne est égal à $p_{i,j} = P_{(X_n = x_i)}(X_{n+1} = x_j)$.</p>\n</div>\n\n<div class=\"formula\">\n<h4>Graphe associé à une chaîne de Markov</h4>\n<p>Soit $(X_n)$ une chaîne de Markov homogène dont on note $E = \\{x_1, x_2, \\dots, x_m\\}$ l’espace des états. On associe à cette chaîne de Markov un graphe probabiliste $G$ d’ordre $m$ dont les sommets sont les états $x_i$ et dont les arêtes $x_i - x_j$ sont pondérées par les poids $p_{i,j} = P_{(X_n = x_i)}(X_{n+1} = x_j)$.</p>\n<p>La matrice de transition de $(X_n)$ est égale à la matrice de transition du graphe probabiliste $G$ : il s’agit donc aussi d’une matrice stochastique.</p>\n</div>\n\n<h3 id=\"distributions-dans-une-chaîne-de-markov\">Distributions dans une chaîne de Markov</h3>\n<div class=\"formula\">\n<h4>Proposition</h4>\n<p>Soit $(X_n)$ une chaîne de Markov homogène dont on note $E = \\{x_1, x_2, \\dots, x_m\\}$ l’espace des états. On pose $p_{i,j}^{(k)} = P_{(X_0 = x_i)}(X_k = x_j)$ pour tout $k \\in \\mathbb{N}^*$ (qui représente la probabilité que la chaîne de Markov $(X_n)$ passe de l’état $x_i$ à l’état $x_j$ en $k$ étapes). On a :</p>\n<p>$\\displaystyle{p_{i,j}^{(k)} = \\sum_{q=1}^m p_{i,q}^{(k-1)} \\times p_{q,j}^{(1)} = p_{i,1}^{(k-1)} \\times p_{1,j}^{(1)} + p_{i,2}^{(k-1)} \\times p_{2,j}^{(1)} + \\dots + p_{i,m}^{(k-1)} \\times p_{m,j}^{(1)}}$.</p>\n<p>De plus, comme $(X_n)$ est homogène, $p_{i,j}^{(k)} = p_{i,j}^{(n+k)}$ pour tout $n \\in \\mathbb{N}$.</p>\n</div>\n\n<p>Cette formule semble un petit peu compliquée à interpréter. Elle signifie simplement que la probabilité que la chaîne de Markov $(X_n)$ passe de l’état $x_i$ à l’état $x_j$ en $k$ étapes est égale à la probabilité qu’elle passe de l’état $e_i$ à $e_q$ en une étape, puis de passer de $e_q$ à $e_j$ en $k-1$ étapes. Heureusement, il est possible de la simplifier grandement à l’aide des matrices de transition.</p>\n<div class=\"formula\">\n<h4>Lien avec la matrice de transition</h4>\n<p>En reprenant les notations précédentes et en notant $M$ la matrice de transition de $(X_n)$, alors $p_{i,j}^{(k)}$ est le coefficient à la ligne $i$ et à la colonne $j$ de la matrice $M^k$.</p>\n</div>\n<p>Enfin, donnons la définition centrale de cette section.</p>\n<div data-api-v2-content-width=\"big\" class=\"formula\">\n<h4>Définition</h4>\n\n<p>Soit $(X_n)$ une chaîne de Markov homogène dont on note $E = \\{x_1, x_2, \\dots, x_m\\}$ l’espace des états. On appelle <strong>suite des distributions</strong> de $(X_n)$ la suite de matrices $(\\pi_n)$, définie pour tout $n \\in \\mathbb{N}$ par $\\displaystyle{\\pi_n = \\begin{pmatrix} P(X_n = x_1) &amp; P(X_n = x_2) &amp; \\dots &amp; P(X_n = e_m) \\end{pmatrix}}$.</p>\n<p>$\\pi_n$ est donc une matrice ligne d’ordre $m$ et est appelée <strong>distribution au temps $n$</strong>.</p>\n<p>$\\pi_0$ (la distribution au temps $0$) est appelée <strong>distribution initiale</strong>.</p>\n</div>\n<p>Une propriété très sympathique des distributions, est que l’on dispose d’une relation de récurrence permettant de calculer facilement la distribution à un temps $n$ donné.</p>\n<div class=\"formula\">\n<h4>Relation entre $\\pi_{n+1}$ et $\\pi_n$</h4>\n<p>En reprenant les notations de la définition précédente et en notant $M$ la matrice de transition de $(X_n)$, alors la suite $(\\pi_n)$ vérifie une relation de récurrence donnée pour tout $n \\in \\mathbb{N}$ par $\\pi_{n+1} = \\pi_n M$.</p>\n<p>On en déduit que pour tout $n \\in \\mathbb{N}$, $\\pi_n = \\pi_0 M^n$.</p>\n</div>\n\n\n<h3 id=\"distribution-invariante\">Distribution invariante</h3>\n<div class=\"formula\">\n<h4>Définition</h4>\n<p>Soit $(X_n)$ une chaîne de Markov homogène de matrice de transition $M$. Une distribution $\\pi$ est <strong>invariante</strong> si les deux conditions suivantes sont respectées :</p>\n<ul>\n<li><p>$\\displaystyle{\\pi M = \\pi}$ (donc si $\\pi$ est une distribution à un temps $n$, on a $\\pi = \\pi_n$ et cette condition se résume à avoir $\\pi_n = \\pi_n M = \\pi_{n+1}$).</p></li>\n<li><p>La somme des coefficients de $\\pi$ vaut $1$.</p></li>\n</ul>\n</div>\n<div class=\"formula\">\n<h4>Existence et unicité de la distribution invariante au temps $n$</h4>\n<p>Soit $(X_n)$ une chaîne de Markov homogène de matrice de transition $M$.</p>\n<p>Si $M$ ne possède aucun coefficient non nul autre que sur sa diagonale, alors $(X_n)$ admet une unique distribution invariante $\\pi$.</p>\n</div>\n<div class=\"formula\">\n<h4>Convergence de la distribution</h4>\n<p>Soit $(X_n)$ une chaîne de Markov homogène dont on note $(\\pi_n)$ la suite des distributions.</p>\n<ul>\n<li><p>Si $(\\pi_n)$ est une suite de matrices convergente, alors elle converge vers une distribution invariante $\\pi$.</p></li>\n<li><p>Si le cardinal de l’ensemble des états de $(X_n)$ est $2$, alors $(\\pi_n)$ est convergente (et converge vers la distribution invariante $\\pi$).</p></li>\n</ul>\n</div>\n\n"}